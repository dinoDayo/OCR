{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bdd736",
   "metadata": {},
   "source": [
    "## def Optical Character Recognition:\n",
    "- Inspired by a project a friend told me about, I want to see if I can use this for parsing hand-written notes. Machine Learning seemed like too big of a concept to wrap my head around when I was first learning how convolutional neural networks can support computer vision. Fast forward to today, and it is amazing that I can leverage these tools with such a simple set of commands. \n",
    "- The tutorial I used to get started with this project can be found [here](https://medium.com/@dprakash05/a-complete-guide-to-build-optical-character-recognition-ocr-in-python-5bf179d47db8), along with some environment setup tips [here](https://guides.library.illinois.edu/c.php?g=347520&p=4121425)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473b4ea",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbd6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up env...\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from pytesseract import Output\n",
    "import cv2\n",
    "\n",
    "# load local vars in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# import our file management tools...\n",
    "from file_helper import helper\n",
    "hlp = helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c737cb07",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805dae1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tens et len S str\n",
      "cme) on LY (eU10___)\n",
      "dl te gtk\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import image file...\n",
    "filename = 'Client Cards v1.jpg'\n",
    "img = np.array(Image.open(hlp.data_path_in + filename))\n",
    "\n",
    "# extract strings\n",
    "text = pytesseract.image_to_string(img)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16221b",
   "metadata": {},
   "source": [
    "#### Text Localization and Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65bdda43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': [1, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 5, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 5], 'page_num': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'block_num': [0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 11, 11, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22], 'par_num': [0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1], 'line_num': [0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1], 'word_num': [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 0, 1, 2, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1], 'left': [0, 0, 0, 0, 0, 7, 7, 7, 7, 102, 102, 102, 102, 27, 27, 27, 27, 148, 148, 148, 148, 30, 30, 30, 30, 27, 27, 27, 27, 26, 26, 26, 26, 36, 36, 36, 32, 95, 146, 182, 240, 36, 36, 112, 148, 174, 88, 88, 116, 134, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 26, 26, 26, 26, 231, 231, 231, 231, 26, 26, 26, 26, 40, 40, 40, 40, 8, 8, 8, 8, 26, 26, 26, 26, 261, 261, 261, 261], 'top': [0, 156, 156, 156, 156, 34, 34, 34, 34, 219, 219, 219, 219, 229, 229, 229, 229, 273, 273, 273, 273, 259, 259, 259, 259, 291, 291, 291, 291, 321, 321, 321, 321, 145, 145, 145, 134, 145, 149, 148, 157, 167, 173, 173, 155, 167, 203, 204, 196, 203, 336, 336, 336, 336, 368, 368, 368, 368, 385, 385, 385, 385, 399, 399, 399, 399, 415, 415, 415, 415, 430, 430, 430, 430, 446, 446, 446, 446, 216, 216, 216, 216, 462, 462, 462, 462, 201, 201, 201, 201, 201, 201, 201, 201, 478, 478, 478, 478, 201, 201, 201, 201], 'width': [300, 5, 5, 5, 5, 287, 287, 287, 287, 181, 181, 181, 181, 253, 253, 253, 253, 43, 43, 43, 43, 252, 252, 252, 252, 245, 245, 245, 245, 256, 256, 256, 256, 246, 246, 221, 42, 23, 28, 5, 17, 246, 70, 17, 18, 108, 101, 13, 14, 55, 256, 256, 256, 256, 255, 255, 255, 255, 248, 248, 248, 248, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 256, 256, 256, 256, 37, 37, 37, 37, 256, 256, 256, 256, 40, 40, 40, 40, 40, 40, 40, 40, 256, 256, 256, 256, 39, 39, 39, 39], 'height': [524, 62, 62, 62, 62, 94, 94, 94, 94, 25, 25, 25, 25, 35, 35, 35, 35, 9, 9, 9, 9, 37, 37, 37, 37, 35, 35, 35, 35, 37, 37, 37, 37, 75, 75, 19, 33, 14, 14, 6, 7, 16, 8, 6, 41, 16, 17, 11, 32, 17, 37, 37, 37, 37, 35, 35, 35, 35, 33, 33, 33, 33, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 280, 280, 280, 280, 35, 35, 35, 35, 295, 295, 295, 295, 295, 295, 295, 295, 34, 34, 34, 34, 295, 295, 295, 295], 'conf': ['-1', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '3.357399', '32.107460', '21.965324', '15.675667', '52.233513', '-1', '18.092194', '57.464722', '68.232468', '0.000000', '-1', '33.499367', '30.846687', '19.337700', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000', '-1', '-1', '-1', '95.000000'], 'text': ['', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', 'Tens', 'et', 'len', 'S', 'str', '', 'cme)', 'on', 'LY', '(eU10___)', '', 'dl', 'te', 'gtk', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', ' ', '', '', '', '']}\n"
     ]
    }
   ],
   "source": [
    "results = pytesseract.image_to_data(img, output_type=Output.DICT)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e256f351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overlay observed text margins on original image\n",
    "for i in range(0, len(results[\"text\"])):\n",
    "    x = results[\"left\"][i]\n",
    "    y = results[\"top\"][i]\n",
    "    \n",
    "    w = results[\"width\"][i]\n",
    "    h = results[\"height\"][i]    \n",
    "    text = results[\"text\"][i]\n",
    "    conf = int(float(results[\"conf\"][i]))   \n",
    "    # conf = confidence in that letter being in that location\n",
    "    if conf > 58:\n",
    "        text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(img, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 200), 2)\n",
    "\n",
    "# display results\n",
    "cv2.imshow(\" \",img)\n",
    "cv2.waitKey(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a69212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closing all open windows \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda0697",
   "metadata": {},
   "source": [
    "#### Next Steps:\n",
    "1. Accuracy needs improving...this may mean training an algorithm to recognize the entries...for each person's type of handwriting.....def faster to do manually for now....\n",
    "2. Better handling for unknown characters, (eU10__), ideally we parse them though\n",
    "3. File management still needs work (gDrive IO + DVC.maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04fc32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
